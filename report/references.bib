@article{DBLP:journals/corr/ShazeerMMDLHD17,
  author     = {Noam Shazeer and
                Azalia Mirhoseini and
                Krzysztof Maziarz and
                Andy Davis and
                Quoc V. Le and
                Geoffrey E. Hinton and
                Jeff Dean},
  title      = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
                Layer},
  journal    = {CoRR},
  volume     = {abs/1701.06538},
  year       = {2017},
  url        = {http://arxiv.org/abs/1701.06538},
  eprinttype = {arXiv},
  eprint     = {1701.06538},
  timestamp  = {Mon, 13 Aug 2018 16:46:11 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/ShazeerMMDLHD17.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-2101-03961,
  author     = {William Fedus and
                Barret Zoph and
                Noam Shazeer},
  title      = {Switch Transformers: Scaling to Trillion Parameter Models with Simple
                and Efficient Sparsity},
  journal    = {CoRR},
  volume     = {abs/2101.03961},
  year       = {2021},
  url        = {https://arxiv.org/abs/2101.03961},
  eprinttype = {arXiv},
  eprint     = {2101.03961},
  timestamp  = {Thu, 21 Jan 2021 14:42:30 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2101-03961.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@misc{jiang2024mixtralexperts,
  title         = {Mixtral of Experts},
  author        = {Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
  year          = {2024},
  eprint        = {2401.04088},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2401.04088}
}

@misc{mu2025comprehensivesurveymixtureofexpertsalgorithms,
  title         = {A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and Applications},
  author        = {Siyuan Mu and Sen Lin},
  year          = {2025},
  eprint        = {2503.07137},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2503.07137}
}

@misc{lo2025closerlookmixtureofexpertslarge,
  title         = {A Closer Look into Mixture-of-Experts in Large Language Models},
  author        = {Ka Man Lo and Zeyu Huang and Zihan Qiu and Zili Wang and Jie Fu},
  year          = {2025},
  eprint        = {2406.18219},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2406.18219}
}

@misc{youtube_video,
  author  = {Alexandre TL},
  title   = { Qu'est-ce que le Mixture of Experts (MoE) ? },
  year    = {2024},
  url     = {https://www.youtube.com/watch?v=_P2T4Oi0VxE},
  urldate = {2026-01-17},
  note    = {YouTube video}
}