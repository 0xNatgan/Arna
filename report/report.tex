\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{float}
\usepackage{subcaption}
\usepackage{tikz}
\geometry{margin=2.5cm}
\usetikzlibrary{shapes.geometric, arrows, positioning, fit, backgrounds}

\title{Rapport de Projet : Classification de News via Mixture of Experts (MoE)}
\author{Votre Nom}
\date{\today}

\begin{document}

\maketitle

\section{Introduction et Revue du Concept de MoE}
L'objectif de ce projet est d'explorer l'architecture \textbf{Mixture of Experts} (MoE) et son application à différentes tâches dans ce projet nous nous sommes intréssés à la classification de texte et d'images. \newline
Le concept de MoE repose sur le constat qu'un modèle unique peut avoir du mal à exceller dans l'ensemble des tâches ou les domaines qui lui sont confiés sans devenir excessivement complexe et coûteux en termes de calcul.
L'idée ppour surmonter la dificulté de modélisation de tâches complexes avec un seul modèle dense est de diviser le modèle en un ensemple de sous-modèle plus petits et spécialisés appelés \textbf{experts}
et d'un mécanisme de routage appelé \textbf{gating network} qui décide quel expert doit traiter chaque entrée.
Ainsi au on cherche à achever des performances similaires voir supérieures à celles d'un modèle dense tout en réduisant le coût calcul global.

\subsection{Architecture Générale d'un MoE}
L'architecture générale d'un modèle MoE peut être représentée comme suit :

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{./img/moe_archi.png}
    \centering
    \caption{Architecture MoE}
    \label{fig:Architecture_Moe}
\end{figure}

\begin{itemize}
    \item \textbf{Le routeur (Gating Network)} :
          C'est un modèle qui décide quel expert doit traiter chaque entrée.
          Le routeur prend l'entrée et produit un ensemble de poids ou de probabilités qui indiquent l'importance relative de chaque expert pour cette entrée.
          Le routeur peut être un réseau de neurones simple ou un modèle plus complexe.
          Le routeur peut utiliser différentes stratégies de routage, comme le routage dur (hard routing) où un seul expert est sélectionné, ou le routage doux (soft routing) où plusieurs experts sont combinés.
          Il existe aussi des variantes plus hybrides avec un une selection d'experts en fonction d'un seuil.

    \item \textbf{Les experts (Experts)} :
          Ce sont des sous-modèles spécialisés qui se concentrent sur des aspects spécifiques de la tâche principale.
          Chaque expert peut être un réseau de neurones, un arbre de décision, ou tout autre type de modèle.
          Certains sont concus spécialement pour une tâche particulière à l'avance d'autres se spécialisent durant l'entrainement.
          Dans certains modèles MoE les experts font tous partie d'un même réseau de neurones ou seulement une partie s'active pour chaque experts. ce qui permet de réduire le cout mémoire du modèle.

    \item \textbf{Combinaison des sorties} :
          Une fois que les experts ont traité l'entrée, leurs sorties sont combinées pour produire la sortie finale du modèle MoE.
          La combinaison peut être une simple moyenne pondérée, une somme, ou une opération plus complexe en fonction des poids fournis par le routeur.

\end{itemize}


\subsection{Load Balancing Loss}

L'un des problèmes classiques des architectures MoE est le \textbf{load balancing} (équilibrage de la charge) entre les experts.
Certains experts sont consultés fréquemment, tandis que d'autres sont rarement ou jamais sollicités. Pour encourager le routeur à sélectionner chaque expert avec une fréquence égale au sein de chaque batch, chaque couche MoE utilise des fonctions de perte auxiliaires.

Le \textbf{Switch Transformer} simplifie cela en une seule fonction de perte auxiliaire. Soit $n$ le nombre d'experts, et pour un batch de requêtes $\{x_1, x_2, \ldots, x_T\}$, la perte auxiliaire est définie par :

\begin{equation}
    \mathcal{L}_{\text{aux}} = n \sum_{i=1}^{n} f_i \cdot P_i
\end{equation}

où :
\begin{itemize}
    \item $f_i = \frac{1}{T} \sum_{t=1}^{T} \mathbb{1}\{\text{argmax}(g(x_t)) = i\}$ est la fraction des tokens routés vers l'expert $i$
    \item $P_i = \frac{1}{T} \sum_{t=1}^{T} p_{i}(x_t)$ est la probabilité moyenne assignée à l'expert $i$ par le routeur
\end{itemize}

Cette perte encourage une distribution uniforme de la charge entre les experts, minimisant ainsi le risque d'\textit{expert collapse}.
\subsection{Avantages des MoE}
\begin{itemize}
    \item \textbf{Efficacité} : Seuls les experts compétents pour une partie spécifique du problème sont utilisés, ce qui permet d'économiser du temps et de la puissance de calcul.
    \item \textbf{Flexibilité} : Il est facile d'ajouter de nouveaux experts ou de modifier leurs spécialités, rendant le système adaptable à différents problèmes.
    \item \textbf{Meilleurs résultats} : Étant donné que chaque expert se concentre sur ce qu'il maîtrise le mieux, la solution globale est généralement plus précise et fiable.
    \item \textbf{Scalabilité} : Permet d'augmenter la capacité du modèle sans augmenter proportionnellement le coût computationnel.
    \item \textbf{Spécialisation} : Chaque expert peut se spécialiser dans un aspect particulier des données.
\end{itemize}

\section{Description des Modèles NLP Implémentés}
\subsection{Bert MoE}
\subsubsection{Architecture}
Notre modèle, \texttt{MoEBertModel}, utilise \textbf{BERT} (\textit{bert-base-uncased}) comme extracteur de caractéristiques (backbone). La sortie du \textit{pooler} de BERT alimente :
\begin{itemize}
    \item Un réseau de routage (couche linéaire) qui génère des poids pour chaque expert.
    \item Une liste de $N$ experts (\texttt{TextExpert}), chacun étant un réseau de neurones \textit{feed-forward} avec normalisation et dropout.
\end{itemize}


\subsubsection{Choix de Conception}
\begin{itemize}
    \item \textbf{Soft Routing} : Nous utilisons un routage "doux" via une fonction Softmax pour permettre un apprentissage stable de tous les experts simultanément.
    \item \textbf{Exploration} : Ajout d'un bruit gaussien sur les scores du router durant l'entraînement pour éviter l'effondrement précoce sur un seul expert.
    \item \textbf{GELU} (Gaussian Error Linear Unit) : en raison de ses performances supérieures dans les tâches NLP par rapport à ReLU.
\end{itemize}

\section{Protocole Expérimental et Hyperparamètres}
Le meilleur entraînement avec BERT unfreeze a été effectué sur le dataset AG News.
\begin{table}[H]
    \centering
    \begin{tabular}{lc}
        \toprule
        Hyperparamètre        & Valeur             \\
        \midrule
        Batch Size            & 64                 \\
        Nombre d'Époques      & 3                  \\
        Learning Rate         & $2 \times 10^{-5}$ \\
        Nombre d'Experts      & 8                  \\
        Architecture Backbone & BERT Base          \\
        Max Length Texte      & 128                \\
        Routing               & Soft               \\
        Top K Experts         & 6                  \\
        Freeze Bert           & False              \\
        Load Balancing Coef   & 0.01               \\
        Load Balancing        & True               \\
        \bottomrule
    \end{tabular}
    \caption{Configuration des hyperparamètres}
\end{table}
Le meilleur entraînement avec BERT freeze a été effectué sur le dataset AG News.
\begin{table}[H]
    \centering
    \begin{tabular}{lc}
        \toprule
        Hyperparamètre        & Valeur             \\
        \midrule
        Batch Size            & 64                 \\
        Nombre d'Époques      & 5                  \\
        Learning Rate         & $3 \times 10^{-5}$ \\
        Nombre d'Experts      & 8                  \\
        Architecture Backbone & BERT Base          \\
        Max Length Texte      & 128                \\
        Routing               & Soft               \\
        Top K Experts         & 6                  \\
        Freeze Bert           & True               \\
        Load Balancing Coef   & 0.5                \\
        Load Balancing        & True               \\
        \bottomrule
    \end{tabular}
    \caption{Configuration des hyperparamètres}
\end{table}
\subsection{Transformer MoE}

\subsection{Transformer MoE Layer}
Nous avons également implémenté un modèle Transformer avec des couches MoE personnalisées. Le modèle, \texttt{MoETransformerModelV2}, utilise des couches d'attention multi-têtes suivies de couches MoE éparses. Chaque couche MoE contient plusieurs experts, chacun étant un réseau feed-forward.

\subsubsection{Architecture MoE Transformer V2}

Notre modèle \texttt{MoETransformerModelV2} suit l'architecture Switch Transformer~ en intégrant des couches Mixture-of-Experts (MoE) \textbf{à l'intérieur} de chaque bloc Transformer.

\paragraph{Embeddings.} Les tokens d'entrée sont convertis en vecteurs via une couche d'embedding de dimension $d_{model}$, additionnés avec des embeddings positionnels apprenables.

\paragraph{Couches MoE Transformer.} Le modèle empile $N$ couches identiques, chacune composée de :
\begin{enumerate}
    \item \textbf{Multi-Head Self-Attention} avec connexion résiduelle et normalisation de couche.
    \item \textbf{MoE Feed-Forward} qui remplace le FFN dense standard :
          \begin{itemize}
              \item Un \textit{réseau de gating} $g(x) = \text{softmax}(W_g x)$ calcule les poids de routage pour $E$ experts.
              \item Seuls les \textit{Top-K} experts sont activés pour chaque token.
              \item La sortie est la somme pondérée : $y = \sum_{i \in \text{Top-K}} g_i(x) \cdot E_i(x)$
          \end{itemize}
\end{enumerate}

\paragraph{Classification.} Après les $N$ couches, une normalisation finale est appliquée, suivie d'un mean-pooling sur les tokens non-masqués et d'un classifieur linéaire.


\begin{table}[H]
    \centering
    \begin{tabular}{ll}
        \toprule
        Paramètre                         & Valeur \\
        \midrule
        Dimension embedding ($d_{model}$) & 256    \\
        Dimension FFN ($d_{ff}$)          & 512    \\
        Nombre de têtes d'attention       & 4      \\
        Nombre de couches ($N$)           & 2      \\
        Nombre d'experts ($E$)            & 8      \\
        Top-K                             & 4      \\
        Dropout                           & 0.3    \\
        \bottomrule
    \end{tabular}
    \caption{Hyperparamètres du MoE Transformer V2.}
\end{table}
\section{Résultats Expérimentaux et Interprétation}
\subsection{NLP}
Pour le modèle NLP, nous avons entrainé plusieurs configurations MoE avec difficiles types de routage et comparé leurs performances à un modèle BERT standard.
Comme modele de base, nous avons utilise BERT base uncased, et nous avons freeze les poids de BERT pendant l'entrainement des modèles MoE pour réduire le cout computationnel.
Lorsque nous entrainons le modele MoE avec BERT freeze, nous obtenons une accuracy proche de 88\%, ce qui est comparable au modele BERT standard. Nous avons egalement observe l'impact dur
load balancing, qui a permis d'ameliorer la repartition de l'utilisation des experts et d'eviter l'effondrement de certains experts.

Nous avons construit 3 modeles differents :
-  Un modele avec Bert comme le Backbone
- Un modele Transformer classique avec une couche MoE
- Un modele Transformer

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../NLP_MoE/metrics/model 2/expert_usage_heatmap.png}
        \caption{Soft routing avec load balancing}
        \label{fig:expert_soft}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../NLP_MoE/metrics/model 1/expert_usage_heatmap.png}
        \caption{Hard routing}
        \label{fig:expert_hard}
    \end{subfigure}
    \caption{Comparaison de l'utilisation des experts selon le type de routage.}
    \label{fig:expert_comparison}
\end{figure}
Nous remarquons que l'usage des experts est assez bien repartis lorsqu'on utilise un routage soft et un load-balancing de 0.5
Cependant lorsqu'on passe à un routage hard, on remarque que l'usage des experts devient plus inégal, ce qui peut indiquer un effondrement de certains experts.

Cependant le routage Hard, peut permettre aux experts de se specialiser davantage, ce qui peut conduire à de meilleures performances dans certains cas.
En comparant le routage hard et soft, on remarque que les experts se specialisent plus avec le routage hard, mais au prix d'une utilisation inégale des experts.
Toutes les figures sont disponibles dans le dossier metrics.

\section{Discussion Critique}
\subsection{Forces}
Capacité de spécialisation et maintien d'une haute précision sur des classes complexes grâce à la modularité des experts.
\subsection{Limites}
Risque de genéralisation des experts sur des sous-ensembles spécifiques du dataset, potentiellement limitant la robustesse globale du modèle.
Risque d'\textbf{Expert Collapse} (effondrement) où seuls quelques experts sont activés, rendant le reste de la capacité du modèle inutile si le router n'est pas bien régularisé.
\subsection{Perspectives}
\subsubsection{NLP}
Utiliser des fonctions de perte de \textit{Load Balancing} plus agressives ou explorer le \textbf{Top-k routing} pour réduire le coût computationnel à l'inférence.
Mixer des experts de différentes architectures (par exemple, CNN, RNN) pour capturer diverses caractéristiques des données textuelles. Cela
pourrait ameliorer la specialisation des experts et la performance globale du modele.
Nous pourrons aussi ajouter du noise au gating ou encore ajouter une couche d'adaptation avant le gating.



\subsection{CNN MoE pour la Classification d'Images du dataset Mnist}
\subsubsection{Architecture}
Notre Gating Network est un CNN simple ou les sorties de la derniere couche convolutive sont aplaties et passées à une couche linéaire pour produire les scores des experts.
Chaque expert est un CNN un peu plus profond que le routeur, avec plusieurs couches convolutives suivies de couches fully connected.
en fonction du type de routage choisi (soft, hard, hybrid), les experts sont sélectionnés et leurs sorties combinées pour produire la sortie finale du modèle.
\subsubsection{Choix de Conception}
\begin{itemize}
\item \textbf{Entrainement} : L'entrainement des experts et du routeur se fait simultanément pour permettre au routeur d'apprendre à sélectionner les experts les plus pertinents en fonction des données d'entrée.
\item \textbf{Parametres d'entrainement} : Durant l'entrainement le routage est effectué en mode soft pour permettre une exploration complète et un entraînement complet des experts.
\item \textbf{Load balancing} : Nous avons implémenté une perte de load balancing pour encourager une utilisation uniforme des experts et éviter un déséquilibrage dans le routage des experts.
\item \textbf{Routage Hybride} : Le routage hybride consiste à selectionner les experts dont les scores depassent un certain seuil, permettant ainsi une flexibilité entre les deux extremes il permet d'avoir un score légerement plus élévé lors des tests mais peut utiliser plus d'experts que le routage hard.
\end{itemize}

\end{document}