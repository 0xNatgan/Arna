{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1601020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer\n",
    "from nlp_moe_model import MoEBertModel, MoETransformerModel, MoETransformerModelV2\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from train import train_epoch, evaluate, NewsDataset,load_data\n",
    "import os\n",
    "from plotting_utils import *\n",
    "\n",
    "from config import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419586a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_type=None):\n",
    "    # Configuration\n",
    "    device = t.device('cuda:0' if t.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"GPU Name: {t.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Count: {t.cuda.device_count()}\")\n",
    "    t.cuda.set_device(0)\n",
    "\n",
    "    entropy_history = []\n",
    "    expert_usage_history = []\n",
    "    # Load data\n",
    "    train_df, test_df, label_encoder = load_data(\n",
    "        '/home/knwldgosint/Documents/School5/Advanced Neural network/project/Arna/dataset/train.csv',\n",
    "        '/home/knwldgosint/Documents/School5/Advanced Neural network/project/Arna/dataset/test.csv'\n",
    "    )\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = NewsDataset(\n",
    "        train_df['Description'].tolist(),\n",
    "        train_df['label_encoded'].tolist(),\n",
    "        tokenizer,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "    test_dataset = NewsDataset(\n",
    "        test_df['Description'].tolist(),\n",
    "        test_df['label_encoded'].tolist(),\n",
    "        tokenizer,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)\n",
    "    model = None\n",
    "    # Model\n",
    "    if model_type == 'bert':\n",
    "        model = MoEBertModel(\n",
    "            pretrained_model_name=MODEL_NAME,\n",
    "            expert_number=NUM_EXPERTS,\n",
    "            output_dim=num_classes,\n",
    "            routing = ROUTING,\n",
    "            top_k = TOP_K,\n",
    "            freeze_bert = FREEZE_BERT,\n",
    "        ).to(device)\n",
    "    elif model_type == 'transformer':\n",
    "        model = MoETransformerModel(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            output_dim=num_classes,\n",
    "            expert_number=NUM_EXPERTS,\n",
    "            routing = ROUTING,\n",
    "            top_k = TOP_K,\n",
    "            num_heads=NUM_HEADS,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            max_len=MAX_LEN,\n",
    "        ).to(device)\n",
    "    elif model_type == 'transformer_v2':\n",
    "        model = MoETransformerModelV2(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            output_dim=num_classes,\n",
    "            expert_number=NUM_EXPERTS,\n",
    "\n",
    "            top_k = TOP_K,\n",
    "            num_heads=NUM_HEADS,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            max_len=MAX_LEN,\n",
    "        ).to(device)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type. Choose from 'bert', 'transformer', 'transformer_v2'.\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = t.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Training loop\n",
    "    best_acc = 0\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    hyperparam = get_hyperparameters_text()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "        train_loss, train_acc,metrics = train_epoch(model, train_loader, optimizer, criterion, device, load_balance=LOAD_BALANCE, balance_coef=LOAD_BALANCE_COEF)\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        entropy_history.append(metrics['avg_entropy'])\n",
    "        if metrics['expert_usage'] is not None:\n",
    "            expert_usage_history.append(metrics['expert_usage'])\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            t.save(model.state_dict(), 'best_moe_model.pth')\n",
    "            print(\"Model saved!\")\n",
    "\n",
    "    print(f\"\\nBest Test Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "    # Generate analytics plots\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Generating Analytics...\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, train_accs, test_losses, test_accs, hyperparams=hyperparam, save_path=\"model_\"+str(model_type)+\"training_history.png\")\n",
    "\n",
    "    # Load best model for final evaluation\n",
    "    model.load_state_dict(t.load('best_moe_model.pth'))\n",
    "\n",
    "    # Get predictions for confusion matrix\n",
    "    _, _, test_predictions, test_labels = evaluate(\n",
    "        model, test_loader, criterion, device, return_predictions=True\n",
    "    )\n",
    "\n",
    "    # Get class names\n",
    "    class_names = [str(cls) for cls in label_encoder.classes_]\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(test_labels, test_predictions, class_names, hyperparams=hyperparam)\n",
    "\n",
    "    # Plot per-class metrics\n",
    "    plot_per_class_metrics(test_labels, test_predictions, class_names, hyperparams=hyperparam)\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(test_labels, test_predictions, target_names=class_names))\n",
    "\n",
    "    # Analyze expert usage per class\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Analyzing Expert Usage...\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    expert_usage = analyze_expert_usage(\n",
    "        model, test_loader, device, NUM_EXPERTS, num_classes\n",
    "    )\n",
    "\n",
    "    # Plot expert usage heatmap\n",
    "    plot_expert_usage_heatmap(expert_usage, class_names, hyperparams=hyperparam,save_path=\"model_\"+str(model_type)+\"expert_usage_heatmap.png\"    )\n",
    "\n",
    "    # Plot expert specialization\n",
    "    plot_expert_specialization(expert_usage, class_names, hyperparams=hyperparam,save_path=\"model_\"+str(model_type)+\"expert_specialization.png\"    )\n",
    "\n",
    "    # Plot expert preference heatmap\n",
    "    plot_expert_preference_heatmap(expert_usage, class_names, hyperparams=hyperparam,save_path=\"model_\"+str(model_type)+\"expert_preference_heatmap.png\"    )\n",
    "    # Plot expert entropy\n",
    "    plot_expert_entropy(expert_usage, hyperparams=hyperparam,save_path=\"model_\"+str(model_type)+\"expert_entropy.png\")\n",
    "\n",
    "    # Plot entropy evolution\n",
    "    plot_gating_entropy_evolution(entropy_history, save_path=\"model_\"+str(model_type)+\"gating_entropy_evolution.png\")\n",
    "\n",
    "    # Plot expert usage evolution\n",
    "    if expert_usage_history:\n",
    "        plot_expert_usage_std_evolution(expert_usage_history, hyperparams=hyperparam,save_path=\"model_\"+str(model_type)+\"expert_usage_std_evolution.png\")\n",
    "        plot_expert_usage_evolution(expert_usage_history, hyperparams=hyperparam,save_path=\"model_\"+str(model_type)+\"expert_usage_evolution.png\")\n",
    "    # Print top experts for each class\n",
    "    print(\"\\nTop 3 Experts per Class:\")\n",
    "    print(\"=\"*50)\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        top_experts = np.argsort(expert_usage[i])[::-1][:3]\n",
    "        print(f\"{class_name}: Experts {top_experts} with weights {expert_usage[i][top_experts]}\")\n",
    "\n",
    "    t.cuda.empty_cache()\n",
    "\n",
    "main(\"transformer_v2\")\n",
    "# main(\"transformer\")\n",
    "# main(\"bert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a5c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_ablation_study():\n",
    "    \"\"\"Compare different configurations\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Configurations to test\n",
    "    configs = [\n",
    "        {'num_experts': 4, 'routing': 'soft', 'dropout': DROPOUT},\n",
    "        {'num_experts': 8, 'routing': 'soft', 'dropout': DROPOUT},\n",
    "        {'num_experts': 4, 'routing': 'hard', 'dropout': DROPOUT},\n",
    "        {'num_experts': 8, 'routing': 'hard', 'dropout': DROPOUT},\n",
    "        {'num_experts': 4, 'routing': 'soft', 'dropout': DROPOUT+0.2},\n",
    "        {'num_experts': 4, 'routing': 'gumbel', 'dropout': DROPOUT},\n",
    "    ]\n",
    "\n",
    "    device = t.device('cuda:0' if t.cuda.is_available() else 'cpu')\n",
    "    train_df, test_df, label_encoder = load_data(\n",
    "        '/home/knwldgosint/Documents/School5/Advanced Neural network/project/Arna/dataset/train.csv',\n",
    "        '/home/knwldgosint/Documents/School5/Advanced Neural network/project/Arna/dataset/test.csv'\n",
    "    )\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    train_dataset = NewsDataset(\n",
    "        train_df['Description'].tolist(),\n",
    "        train_df['label_encoded'].tolist(),\n",
    "        tokenizer, max_length=MAX_LEN\n",
    "    )\n",
    "    test_dataset = NewsDataset(\n",
    "        test_df['Description'].tolist(),\n",
    "        test_df['label_encoded'].tolist(),\n",
    "        tokenizer, max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=4)\n",
    "\n",
    "    for config in configs:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing: {config}\")\n",
    "        print('='*50)\n",
    "\n",
    "        # model = MoEBertModel(\n",
    "        #     pretrained_model_name=MODEL_NAME,\n",
    "        #     expert_number=config['num_experts'],\n",
    "        #     output_dim=len(label_encoder.classes_),\n",
    "        #     routing=config['routing'],\n",
    "        #     top_k=2,\n",
    "        #     freeze_bert=True,\n",
    "        #     dropout=config['dropout']\n",
    "        # ).to(device)\n",
    "\n",
    "        model = MoETransformerModelV2(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            output_dim=len(label_encoder.classes_),\n",
    "            expert_number=config['num_experts'],\n",
    "            top_k=2,\n",
    "            num_heads=NUM_HEADS,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            max_len=MAX_LEN,\n",
    "            dropout=config['dropout']\n",
    "        ).to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = t.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        # Train for fewer epochs for ablation\n",
    "        entropy_history = []\n",
    "        for epoch in range(2):\n",
    "            train_loss, train_acc, metrics = train_epoch(\n",
    "                model, train_loader, optimizer, criterion, device,\n",
    "                load_balance=True, balance_coef=0.1\n",
    "            )\n",
    "            entropy_history.append(metrics['avg_entropy'])\n",
    "\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "        # Analyze expert usage\n",
    "        expert_usage = analyze_expert_usage(\n",
    "            model, test_loader, device, config['num_experts'], len(label_encoder.classes_)\n",
    "        )\n",
    "\n",
    "        # Compute specialization score (inverse of entropy)\n",
    "        expert_dist = expert_usage / (expert_usage.sum(axis=0, keepdims=True) + 1e-10)\n",
    "        expert_entropy = -np.sum(expert_dist * np.log(expert_dist + 1e-10), axis=0)\n",
    "        avg_specialization = 1 - (expert_entropy.mean() / np.log(len(label_encoder.classes_)))\n",
    "\n",
    "        results.append({\n",
    "            **config,\n",
    "            'test_acc': test_acc,\n",
    "            'avg_entropy': np.mean(entropy_history),\n",
    "            'specialization': avg_specialization\n",
    "        })\n",
    "\n",
    "        t.cuda.empty_cache()\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "df = run_ablation_study()\n",
    "plot_ablation_results(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cf3fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_moe_model import BertDenseBaseline\n",
    "def run_ablation_study2():\n",
    "    \"\"\"Compare different configurations including dense baseline\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Configurations to test\n",
    "    configs = [\n",
    "        # Baselines (no MoE)\n",
    "        {'type': 'dense', 'hidden_mult': 1, 'dropout': DROPOUT},  # Simple baseline\n",
    "        # {'type': 'dense', 'hidden_mult': 4, 'dropout': DROPOUT},  # Equivalent  to 4 experts\n",
    "        # {'type': 'dense', 'hidden_mult': 8, 'dropout': DROPOUT},  # Equivalent  to 8 experts\n",
    "\n",
    "        # # MoE configurations\n",
    "        {'type': 'moe', 'num_experts': 4, 'routing': 'soft', 'dropout': DROPOUT},\n",
    "\n",
    "        # {'type': 'moe', 'num_experts': 8, 'routing': 'soft', 'dropout': DROPOUT},\n",
    "        # {'type': 'moe', 'num_experts': 4, 'routing': 'soft', 'dropout': DROPOUT},\n",
    "    ]\n",
    "\n",
    "    device = t.device('cuda:0' if t.cuda.is_available() else 'cpu')\n",
    "    train_df, test_df, label_encoder = load_data(\n",
    "        '/home/knwldgosint/Documents/School5/Advanced Neural network/project/Arna/dataset/train.csv',\n",
    "        '/home/knwldgosint/Documents/School5/Advanced Neural network/project/Arna/dataset/test.csv'\n",
    "    )\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    train_dataset = NewsDataset(\n",
    "        train_df['Description'].tolist(),\n",
    "        train_df['label_encoded'].tolist(),\n",
    "        tokenizer, max_length=MAX_LEN\n",
    "    )\n",
    "    test_dataset = NewsDataset(\n",
    "        test_df['Description'].tolist(),\n",
    "        test_df['label_encoded'].tolist(),\n",
    "\n",
    "        tokenizer, max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=4)\n",
    "\n",
    "    for config in configs:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing: {config}\")\n",
    "        print('='*50)\n",
    "\n",
    "        if config['type'] == 'dense':\n",
    "            from nlp_moe_model import BertDenseBaseline\n",
    "            model = BertDenseBaseline(\n",
    "                pretrained_model_name=MODEL_NAME,\n",
    "                output_dim=len(label_encoder.classes_),\n",
    "                hidden_multiplier=config['hidden_mult'],\n",
    "                freeze_bert=True,\n",
    "                dropout=config['dropout']\n",
    "            ).to(device)\n",
    "            config_name = f\"Dense_x{config['hidden_mult']}\"\n",
    "        else:\n",
    "            model = MoETransformerModelV2(\n",
    "                vocab_size=tokenizer.vocab_size,\n",
    "                embedding_dim=EMBEDDING_DIM,\n",
    "                hidden_dim=HIDDEN_DIM,\n",
    "                output_dim=len(label_encoder.classes_),\n",
    "                expert_number=config['num_experts'],\n",
    "                top_k = 2,\n",
    "                num_heads=NUM_HEADS,\n",
    "                num_layers=NUM_LAYERS,\n",
    "                max_len=MAX_LEN,\n",
    "                routing=config['routing'],\n",
    "                dropout=config['dropout']\n",
    "            ).to(device)\n",
    "            config_name = f\"MoE_{config['num_experts']}E_{config['routing']}\"\n",
    "\n",
    "        # Count parameters\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = t.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        for epoch in range(2):\n",
    "            if config['type'] == 'moe':\n",
    "                train_loss, train_acc, metrics = train_epoch(\n",
    "                    model, train_loader, optimizer, criterion, device,\n",
    "                    load_balance=True, balance_coef=0.1\n",
    "                )\n",
    "            else:\n",
    "\n",
    "                train_loss, train_acc, _ = train_epoch(\n",
    "                    model, train_loader, optimizer, criterion, device,\n",
    "                    load_balance=False\n",
    "                )\n",
    "\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "        results.append({\n",
    "            'config_name': config_name,\n",
    "            'type': config['type'],\n",
    "            'test_acc': test_acc,\n",
    "            'trainable_params': trainable_params,\n",
    "            **{k: v for k, v in config.items() if k != 'type'}\n",
    "        })\n",
    "\n",
    "        t.cuda.empty_cache()\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "df = run_ablation_study2()\n",
    "plot_ablation_results_v2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b61e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load balance vs no load balance comparison\n",
    "def load_balance_comparison():\n",
    "    results = []\n",
    "\n",
    "    device = t.device('cuda:0' if t.cuda.is_available() else 'cpu')\n",
    "    train_df, test_df, label_encoder = load_data(\n",
    "        '/home/knwldgosint/Documents/School5/Advanced Neural network/project/Arna/dataset/train.csv',\n",
    "        '/home/knwldgosint/Documents/School5/Advanced Neural network/project/Arna/dataset/test.csv'\n",
    "    )\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    train_dataset = NewsDataset(\n",
    "        train_df['Description'].tolist(),\n",
    "        train_df['label_encoded'].tolist(),\n",
    "        tokenizer, max_length=MAX_LEN\n",
    "    )\n",
    "    test_dataset = NewsDataset(\n",
    "        test_df['Description'].tolist(),\n",
    "        test_df['label_encoded'].tolist(),\n",
    "        tokenizer, max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=4)\n",
    "\n",
    "    for load_balance in [True, False]:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing Load Balance: {load_balance}\")\n",
    "        print('='*50)\n",
    "\n",
    "        model = MoEBertModel(\n",
    "            pretrained_model_name=MODEL_NAME,\n",
    "            expert_number=8,\n",
    "            output_dim=len(label_encoder.classes_),\n",
    "            routing='soft',\n",
    "            top_k=2,\n",
    "            freeze_bert=True,\n",
    "            dropout=0.3\n",
    "        ).to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = t.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        for epoch in range(2):\n",
    "            train_loss, train_acc, metrics = train_epoch(\n",
    "                model, train_loader, optimizer, criterion, device,\n",
    "                load_balance=load_balance, balance_coef=0.1\n",
    "            )\n",
    "\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "        results.append({\n",
    "            'load_balance': load_balance,\n",
    "            'test_acc': test_acc,\n",
    "        })\n",
    "\n",
    "        t.cuda.empty_cache()\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "df = load_balance_comparison()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fa44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_balance_results(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rocm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
